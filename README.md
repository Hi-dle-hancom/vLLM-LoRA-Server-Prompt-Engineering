# vLLM LoRA Server - Prompt Engineering

ê³ ì„±ëŠ¥ vLLM ê¸°ë°˜ LoRA ì–´ëŒ‘í„° ì„œë¹™ ì‹œìŠ¤í…œìœ¼ë¡œ, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ ì™„ì „í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

- **vLLM ì—”ì§„**: ê³ ì„±ëŠ¥ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì¶”ë¡ 
- **LoRA ì–´ëŒ‘í„° ì§€ì›**: ë™ì  ì–´ëŒ‘í„° ë¡œë”© ë° ê´€ë¦¬
- **í”„ë¡¬í”„íŠ¸ ë¹Œë”**: ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì§€ì›
- **Docker ì»¨í…Œì´ë„ˆí™”**: ì™„ì „í•œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
- **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana í†µí•©
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL ì§€ì›
- **RESTful API**: FastAPI ê¸°ë°˜ ê³ ì„±ëŠ¥ API

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
vllm-lora-server/
â”œâ”€â”€ app/                    # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ main.py            # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ engine.py          # vLLM ì—”ì§„ ê´€ë¦¬
â”‚   â”œâ”€â”€ prompt_builder.py  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¹Œë”
â”‚   â”œâ”€â”€ schemas.py         # Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜
â”‚   â””â”€â”€ config.py          # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ postgres_init/         # PostgreSQL ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ grafana/              # Grafana ì„¤ì •
â”œâ”€â”€ docker-compose.yml    # Docker Compose ì„¤ì •
â”œâ”€â”€ Dockerfile           # vLLM ì„œë²„ Docker ì´ë¯¸ì§€
â”œâ”€â”€ requirements.txt     # Python ì˜ì¡´ì„±
â””â”€â”€ prometheus.yml       # Prometheus ì„¤ì •
```

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Backend**: FastAPI, vLLM, PyTorch
- **Database**: PostgreSQL
- **Monitoring**: Prometheus, Grafana
- **Containerization**: Docker, Docker Compose
- **AI/ML**: Transformers, LoRA Adapters

## ğŸ”§ ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. í™˜ê²½ ìš”êµ¬ì‚¬í•­
- Docker & Docker Compose
- NVIDIA GPU (CUDA ì§€ì›)
- 8GB+ GPU ë©”ëª¨ë¦¬ ê¶Œì¥

### 2. ì‹¤í–‰
```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone git@github.com:Hi-dle-hancom/vLLM-LoRA-Server-Prompt-Engineering.git
cd vLLM-LoRA-Server-Prompt-Engineering

# Docker Composeë¡œ ì „ì²´ ìŠ¤íƒ ì‹¤í–‰
docker-compose up -d
```

### 3. ì„œë¹„ìŠ¤ ì ‘ì†
- **vLLM API**: http://localhost:8002
- **Grafana**: http://localhost:3000
- **Prometheus**: http://localhost:9090

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### ê¸°ë³¸ ì¶”ë¡ 
```bash
POST /v1/completions
Content-Type: application/json

{
  "prompt": "def fibonacci(n):",
  "max_tokens": 100,
  "temperature": 0.7
}
```

### LoRA ì–´ëŒ‘í„° ì‚¬ìš©
```bash
POST /v1/completions
Content-Type: application/json

{
  "prompt": "ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”:",
  "lora_adapter": "code-explanation",
  "max_tokens": 200
}
```

## ğŸ” ëª¨ë‹ˆí„°ë§

- **Prometheus**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥
- **Grafana**: ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
- **ë¡œê·¸**: Docker ì»¨í…Œì´ë„ˆ ë¡œê·¸ í†µí•©

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â–¶â”‚  vLLM Server    â”‚â”€â”€â”€â–¶â”‚  LoRA Adapters  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Grafana       â”‚â—€â”€â”€â”€â”‚  Prometheus     â”‚â—€â”€â”€â”€â”‚  PostgreSQL     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ê°œë°œ ë° ë°°í¬

### ê°œë°œ ëª¨ë“œ
```bash
# ê°œë°œìš© ì‹¤í–‰ (ë³¼ë¥¨ ë§ˆìš´íŠ¸)
docker-compose -f docker-compose.yml up --build
```

### í”„ë¡œë•ì…˜ ë°°í¬
```bash
# í”„ë¡œë•ì…˜ ëª¨ë“œ
docker-compose -f docker-compose.yml up -d --build
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” HancomAI ì•„ì¹´ë°ë¯¸ì˜ ë‚´ë¶€ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

**Hancom AI Team** - vLLM LoRA Server Prompt Engineering Solution