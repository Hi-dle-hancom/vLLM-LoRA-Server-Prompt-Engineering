# app/engine.py
"""
vLLM ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  í…ìŠ¤íŠ¸ ìƒì„± ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì½”ì–´ ëª¨ë“ˆ.
VRAM ìµœì í™”, ë²ˆì—­ ì„œë¹„ìŠ¤ ì—°ë™, ìƒì„± ì¤‘ë‹¨ ë¡œì§ì´ ëª¨ë‘ í¬í•¨ëœ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.
"""
import logging
import time
import json
from typing import Dict, List, Any, AsyncGenerator
import httpx  # API í˜¸ì¶œì„ ìœ„í•œ httpx

from fastapi import HTTPException
from transformers import AutoTokenizer
from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.lora.request import LoRARequest

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from .config import ModelConfig
from .schemas import GenerateRequest, StreamGenerateRequest
# âœ¨ prompt_builder.pyë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
# from .prompt_builder import generate_enhanced_prompt

logger = logging.getLogger(__name__)

# ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ ì£¼ì†Œ (docker-compose ì„œë¹„ìŠ¤ ì´ë¦„ ê¸°ì¤€)
TRANSLATOR_API_URL = "http://translator:8003/translate"

async def call_translation_service(korean_prompt: str) -> str:
    """ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œì‘: '{korean_prompt}'")
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(TRANSLATOR_API_URL, json={"text": korean_prompt})
            response.raise_for_status()
            data = response.json()
            translated_text = data.get('translated_text', korean_prompt)
            logger.info(f"ë²ˆì—­ ì™„ë£Œ: -> '{translated_text}'")
            return translated_text
    except httpx.RequestError as e:
        logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤({e.request.url}) í˜¸ì¶œ ì‹¤íŒ¨: {e}. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return korean_prompt

class VLLMMultiLoRAEngine:
    """vLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—”ì§„ í´ë˜ìŠ¤"""

    def __init__(self, base_model_path: str, model_configs: Dict[str, ModelConfig]):
        self.base_model_path = base_model_path
        self.model_configs = model_configs
        self.engine = None
        self.tokenizer = None
        self.is_initialized = False

    def initialize_engine(self):
        """vLLM ë¹„ë™ê¸° ì—”ì§„ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ vLLM ë¹„ë™ê¸° ì—”ì§„ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘ (VRAM ìµœì í™” ëª¨ë“œ)...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            
            # âœ¨ ì•ˆì •ì„± ìš°ì„  EngineArgs ì„¤ì • (í† í° ì¤‘ë³µ ë°©ì§€)
            engine_args = AsyncEngineArgs(
                model=self.base_model_path,
                # quantization="marlin",  # ë¹„í™œì„±í™”: í† í° ì¤‘ë³µ ì›ì¸ ê°€ëŠ¥ì„±
                enable_lora=True,
                trust_remote_code=True,
                gpu_memory_utilization=0.5,  # 0.4 â†’ 0.5 ì¦ê°€
                max_model_len=4096,  # 2048 â†’ 4096 ì¦ê°€ (ë” ê¸´ ì»´í…ìŠ¤íŠ¸)
                max_num_seqs=2,  # 4 â†’ 2 ê°ì†Œ (ì•ˆì •ì„± ìš°ì„ )
                tokenizer_mode="auto",  # "slow" â†’ "auto" ë³€ê²½
                max_loras=3,  # 5 â†’ 3 ê°ì†Œ
                max_lora_rank=16,  # 32 â†’ 16 ê°ì†Œ
                dtype="half",
                tensor_parallel_size=1,
                # ì¶”ê°€ ì•ˆì •ì„± ì˜µì…˜
                enforce_eager=True,  # CUDA ê·¸ë˜í”„ ìµœì í™” ë¹„í™œì„±í™”
                disable_custom_all_reduce=True,  # ì»¤ìŠ¤í…€ reduce ë¹„í™œì„±í™”
            )
            
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            self.is_initialized = True
            logger.info("âœ… ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (VRAM ìµœì í™” ëª¨ë“œ)")
            
            # ëª¨ë¸ ìƒíƒœ ê°„ë‹¨ ê²€ì¦
            logger.info("ğŸ” ëª¨ë¸ ìƒíƒœ ê²€ì¦ ì‹œì‘...")
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
                test_prompt = "Hello"
                test_params = SamplingParams(temperature=0.1, max_tokens=5)
                # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ëŠ” ì—¬ê¸°ì„œ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ (ì´ˆê¸°í™” ë‹¨ê³„ì—ì„œëŠ” ìœ„í—˜)
                logger.info("âœ… ëª¨ë¸ ê¸°ë³¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âš ï¸ ëª¨ë¸ ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.error(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self.is_initialized = False
            raise

    def _build_chatml_prompt(self, config: ModelConfig, user_prompt: str, model_type: str) -> str:
        """ChatML í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        
        # FIM íƒœê·¸ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬
        is_fim_request = "<ï½œfim beginï½œ>" in user_prompt or "<|fim_begin|>" in user_prompt
        
        if is_fim_request:
            logger.info("FIM ìš”ì²­ ê°ì§€ë¨ - ì½”ë“œ ì™„ì„± ëª¨ë“œ í™œì„±í™”")
            # FIM ìš”ì²­ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
            user_prompt = self._enhance_fim_prompt(user_prompt)
        
        # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if model_type == "autocomplete":
            system_content = f"""
You are an expert Python code completion assistant.

Your task:
- Complete the user's code snippet by filling in the missing parts
- Provide only the necessary code to complete the functionality
- Use proper Python syntax and best practices
- Keep completions concise and relevant

Output format:
- Provide ONLY the completed code
- Do NOT add explanations or comments unless specifically requested
- Always end with: # --- Generation Complete ---

Remember: You are completing code, not writing essays.
""".strip()
        
        elif model_type == "prompt":
            system_content = f"""
You are a helpful senior Python developer with expertise in writing clean, efficient code.

Your task:
- Generate Python code based on the user's request
- Provide working, tested code solutions
- Follow Python best practices and PEP 8 guidelines
- Include necessary imports and error handling

Output format:
1. First, provide the complete code solution
2. Then, on a new line, write '--- Rationale ---' followed by a brief explanation
3. Always end with: # --- Generation Complete ---

Remember: Focus on practical, working solutions.
""".strip()
        
        elif model_type == "comment":
            system_content = f"""
You are a Python code documentation specialist.

Your task:
- Generate clear, concise comments and docstrings
- Explain code functionality and purpose
- Follow Python documentation standards
- Use proper docstring formats (Google/NumPy style)

Output format:
- Provide the requested comments/docstrings
- Keep explanations clear and technical
- Always end with: # --- Generation Complete ---

Remember: Documentation should be helpful and accurate.
""".strip()
        
        elif model_type == "error_fix":
            system_content = f"""
You are a Python debugging expert specializing in error analysis and fixes.

Your task:
- Analyze the provided code and identify issues
- Provide corrected code with fixes applied
- Explain what was wrong and how it was fixed
- Ensure the solution handles edge cases

Output format:
1. First, provide the corrected code
2. Then, write '--- Fix Explanation ---' followed by what was wrong and how you fixed it
3. Always end with: # --- Generation Complete ---

Remember: Focus on robust, error-free solutions.
""".strip()
        
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_content = config.system_prompt
        
        # ChatML í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
        chatml_prompt = f"""<|im_start|>system
{system_content}<|im_end|>
<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""
        
        logger.debug(f"ChatML í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ (ëª¨ë¸: {model_type}) - í›ˆë ¨ ë°ì´í„° í˜•ì‹ í˜¸í™˜")
        return chatml_prompt

    def _enhance_fim_prompt(self, user_prompt: str) -> str:
        """FIM ìš”ì²­ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
        # FIM íƒœê·¸ë¥¼ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¡œ ë³€í™˜
        enhanced_prompt = user_prompt
        
        # ë‹¤ì–‘í•œ FIM íƒœê·¸ í˜•ì‹ ì§€ì›
        fim_patterns = [
            ("<ï½œfim beginï½œ>", "[CODE_START]"),
            ("<ï½œfim holeï½œ>", "[FILL_HERE]"),
            ("<ï½œfim endï½œ>", "[CODE_END]"),
            ("<|fim_begin|>", "[CODE_START]"),
            ("<|fim_hole|>", "[FILL_HERE]"),
            ("<|fim_end|>", "[CODE_END]")
        ]
        
        for old_tag, new_tag in fim_patterns:
            enhanced_prompt = enhanced_prompt.replace(old_tag, new_tag)
        
        # FIM ìš”ì²­ì— ëŒ€í•œ ìì—°ì–´ ì„¤ëª… ì¶”ê°€ (í›ˆë ¨ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ í•œêµ­ì–´ ì‚¬ìš©)
        if "[FILL_HERE]" in enhanced_prompt:
            enhanced_prompt = f"""ì£¼ì„ì— ë”°ë¼ ì ì ˆí•œ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì´ì „ ì½”ë“œ:
{enhanced_prompt.replace('[FILL_HERE]', '// ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‚½ì…í•´ì•¼ í•¨ //')}

ìœ„ ì½”ë“œì—ì„œ ë¹„ì–´ìˆëŠ” ë¶€ë¶„ì„ ì±„ì›Œì£¼ì„¸ìš”. Python ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¥´ê³  ì£¼ë³€ ì½”ë“œì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ í•´ì£¼ì„¸ìš”."""
        
        return enhanced_prompt

    def _get_stop_token_ids(self, stop_strings: List[str]) -> List[int]:
        """ìŠ¤íƒ‘ ë¬¸ìì—´ì„ í† í° IDë¡œ ë³€í™˜"""
        stop_token_ids = []
        
        if self.tokenizer is None:
            return stop_token_ids
            
        # EOS í† í° ë° íŠ¹ìˆ˜ í† í° ê°•ì œ ì¶”ê°€
        special_tokens = [
            self.tokenizer.eos_token_id,  # EOS í† í°
            self.tokenizer.pad_token_id,  # PAD í† í°
        ]
        
        for token_id in special_tokens:
            if token_id is not None:
                stop_token_ids.append(token_id)
            
        for stop_str in stop_strings:
            try:
                # ë¬¸ìì—´ì„ í† í° IDë¡œ ë³€í™˜
                token_ids = self.tokenizer.encode(stop_str, add_special_tokens=False)
                stop_token_ids.extend(token_ids)
                
                # ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš© (ë” ì •í™•í•œ ë§¤ì¹­)
                if token_ids:
                    stop_token_ids.append(token_ids[-1])
                    
            except Exception as e:
                logger.warning(f"ìŠ¤íƒ‘ í† í° '{stop_str}' ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        stop_token_ids = list(set(stop_token_ids))
        logger.info(f"ìŠ¤íƒ‘ í† í° ID: {stop_token_ids}")
        
        return stop_token_ids

    async def generate_stream(self, request: StreamGenerateRequest) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_initialized or self.engine is None or self.tokenizer is None:
            raise HTTPException(status_code=503, detail="Model engine not initialized")

        if request.model_type not in self.model_configs:
            error_msg = f"Model type '{request.model_type}' not configured."
            logger.error(error_msg)
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
            return
            
        english_prompt = await call_translation_service(request.prompt)

        config = self.model_configs[request.model_type]
        
        # âœ¨ ChatML í˜•ì‹ìœ¼ë¡œ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìŠ¤íƒ‘ í† í° ì¸ì‹ ê°œì„ )
        final_prompt = self._build_chatml_prompt(config, english_prompt, request.model_type)

        # í† í° ID ê¸°ë°˜ ìŠ¤íƒ‘ í† í° ê³„ì‚°
        stop_token_ids = self._get_stop_token_ids(config.stop or [])
        
        sampling_params = SamplingParams(
            temperature=request.temperature if request.temperature is not None else config.temperature,
            top_p=request.top_p if request.top_p is not None else config.top_p,
            max_tokens=request.max_tokens,
            stop=config.stop or [],
            stop_token_ids=stop_token_ids,  # í† í° ID ê¸°ë°˜ ìŠ¤íƒ‘
            repetition_penalty=1.3,  # ë°˜ë³µ ë°©ì§€ ë” ê°•í™”
            frequency_penalty=0.2,   # ë¹ˆë„ ê¸°ë°˜ í˜ë„í‹° ì¦ê°€
            presence_penalty=0.2,    # ì¡´ì¬ ê¸°ë°˜ í˜ë„í‹° ì¦ê°€
            include_stop_str_in_output=False,  # stop í† í°ì„ ì¶œë ¥ì—ì„œ ì œì™¸
            skip_special_tokens=False  # íŠ¹ìˆ˜ í† í° ìœ ì§€
        )
        
        # FIM íƒœê·¸ ì¸ì‹ì„ ìœ„í•œ ë¡œê¹…
        logger.info(f"Stop tokens for {request.model_type}: {config.stop or []}")
        
        # request_id ìƒì„± (ì‚¬ìš© ì „ì— ë¨¼ì € ì •ì˜)
        request_id = f"stream-{int(time.time() * 1000)}"
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘
        logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘: request_id={request_id}")
        last_text = ""
        token_count = 0
        lora_request = LoRARequest(lora_name=config.name, lora_int_id=config.lora_id, lora_path=config.adapter_path)
        results_generator = self.engine.generate(final_prompt, sampling_params, request_id, lora_request)

        # âœ¨ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ë£¨í”„
        async for request_output in results_generator:
            token_count += 1
            current_text = request_output.outputs[0].text
            
            # 1. ìŠ¤í†± í† í° ì²´í¬
            stop_found = False
            for stop_str in (config.stop or []):
                if stop_str in current_text:
                    current_text = current_text.split(stop_str)[0]
                    stop_found = True
                    logger.info(f"ìŠ¤í†± í† í° '{stop_str}' ê°ì§€ë¨")
                    break
            
            # 2. ë¸íƒ€ ê³„ì‚° (ì¤‘ë³µ ë°©ì§€)
            delta = current_text[len(last_text):] if len(current_text) > len(last_text) else ""
            
            # 3. ë¸íƒ€ í’ˆì§ˆ ê²€ì¦ ë° ì •ë¦¬
            if delta and not stop_found:
                # ë¸íƒ€ í’ˆì§ˆ ê²€ì¦
                cleaned_delta = self._validate_and_clean_delta(delta)
                if cleaned_delta:
                    logger.info(f"ë¸íƒ€ ì „ì†¡: '{cleaned_delta[:50]}...'")
                    yield f"data: {json.dumps({'text': cleaned_delta})}\n\n"
                    last_text = current_text  # ì¤‘ìš”: ì „ì†¡ í›„ ì—…ë°ì´íŠ¸
                else:
                    logger.warning(f"ë¶€ì ì ˆí•œ ë¸íƒ€ í•„í„°ë§: '{delta[:30]}...'")
            
            # 4. ì¢…ë£Œ ì¡°ê±´ ì²´í¬
            if stop_found:
                logger.info("ìŠ¤í†± í† í°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                break
                
            # 5-1. ìµœëŒ€ í† í° ìˆ˜ ì²´í¬
            if len(current_text.split()) >= request.max_tokens:
                logger.info(f"ìµœëŒ€ í† í° ìˆ˜({request.max_tokens}) ë„ë‹¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                break
                
            # 5-2. ë¬¸ì ìˆ˜ ê¸°ë°˜ ì¢…ë£Œ (ëª¨ë¸ íƒ€ì…ë³„) - ë” ê¸´ ì‘ë‹µ í—ˆìš©
            max_chars = {
                "autocomplete": 2000,   # 1000 â†’ 2000
                "prompt": 8000,        # 4000 â†’ 8000 (ë” ê¸´ ì½”ë“œ ìƒì„±)
                "comment": 3000,       # 1500 â†’ 3000
                "error_fix": 6000      # 3000 â†’ 6000
            }.get(request.model_type, 4000)  # ê¸°ë³¸ê°’ë„ 2000 â†’ 4000
            
            if len(current_text) >= max_chars:
                logger.info(f"ë¬¸ì ìˆ˜ ì œí•œ({max_chars}) ë„ë‹¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ (current={len(current_text)})")
                break
            
            # ì£¼ê¸°ì ì¸ ì§„í–‰ ìƒí™© ë¡œê·¸
            if token_count % 10 == 0:
                logger.debug(f"ì§„í–‰ ìƒí™©: tokens={token_count}, chars={len(current_text)}, max_chars={max_chars}")

        # ì •ìƒ ì¢…ë£Œ ì‹œ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
        logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì •ìƒ ì¢…ë£Œ - ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ (token_count={token_count})")
        yield f"data: {json.dumps({'type': 'done', 'text': ''})}\n\n"

    async def generate_single(self, request: GenerateRequest) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        config = self.model_configs[request.model_type]
        stream_request = StreamGenerateRequest(
            user_id=0,
            prompt=request.prompt,
            model_type=request.model_type,
            user_select_options=request.user_select_options,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens if request.max_tokens is not None else config.max_tokens,
        )

        full_text = ""
        async for chunk in self.generate_stream(stream_request):
            # ìƒˆë¡œìš´ JSON í˜•ì‹ ì™„ë£Œ ì‹ í˜¸ ì²˜ë¦¬
            if chunk.strip().startswith("data: "):
                try:
                    data_str = chunk.strip()[5:].strip()
                    if data_str:
                        data = json.loads(data_str)
                        
                        # ì™„ë£Œ ì‹ í˜¸ ì²´í¬
                        if data.get("type") == "done":
                            logger.info("ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                            break
                            
                        # í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
                        if "text" in data and data["text"]:
                            full_text += data["text"]
                        elif "error" in data:
                            logger.error(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ ì˜¤ë¥˜ ìˆ˜ì‹ : {data['error']}")
                            return {'error': data['error']}
                except json.JSONDecodeError:
                    logger.warning(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ JSON ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ: {chunk}")
                    continue
            # êµ¬ì‹ [DONE] ì‹ í˜¸ í˜¸í™˜ì„± ìœ ì§€
            elif chunk.strip() == "data: [DONE]":
                logger.info("êµ¬ì‹ [DONE] ì‹ í˜¸ ìˆ˜ì‹ ")
                break
        return {'generated_text': full_text}

    def _validate_and_clean_delta(self, delta: str) -> str:
        """ë¸íƒ€ í’ˆì§ˆ ê²€ì¦ ë° ì •ë¦¬ (ê°•í™”ëœ ë²„ì „)"""
        if not delta or len(delta.strip()) == 0:
            return ""
        
        import re
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        cleaned = delta.strip()
        
        # 2. ë¹„ì¸ì‡„ ê°€ëŠ¥ ë¬¸ì ì œê±°
        cleaned = ''.join(c for c in cleaned if c.isprintable() or c.isspace())
        
        # 3. ì¤‘ë³µ ë¬¸ì íŒ¨í„´ ì œê±°
        cleaned = re.sub(r'([a-zA-Z_])\1{2,}', r'\1', cleaned)  # ê°™ì€ ë¬¸ì 3ê°œ ì´ìƒ â†’ 1ê°œ
        cleaned = re.sub(r'([a-zA-Z_]{2,})\1+', r'\1', cleaned)  # íŒ¨í„´ ë°˜ë³µ ì œê±°
        
        # 4. ê¸°ê·¸ ë¬¸ì íŒ¨í„´ ê°ì§€ ë° ì°¨ë‹¨
        broken_patterns = [
            r'[\x00-\x1f\x7f-\x9f]{2,}',  # ì œì–´ ë¬¸ì
            r'([a-zA-Z])\1{4,}',  # 5ê°œ ì´ìƒ ë°˜ë³µ
            r'([a-zA-Z]{2,})\1{3,}',  # íŒ¨í„´ 4ë²ˆ ì´ìƒ ë°˜ë³µ
            r'^[^a-zA-Z0-9\s\(\)\[\]\{\}\.,;:"\'\_\-\+\*\/\=\<\>\!\?\#\$\%\&\@\^\`\~\|\\]{3,}',  # ì´ìƒí•œ ë¬¸ìë¡œë§Œ êµ¬ì„±
        ]
        
        for pattern in broken_patterns:
            if re.search(pattern, cleaned):
                logger.warning(f"ê¸°ê·¸ íŒ¨í„´ ê°ì§€ë¡œ ë¸íƒ€ ì°¨ë‹¨: '{pattern}' in '{cleaned[:30]}...'")
                return ""  # ê¸°ê·¹ ë¬¸ì ê°ì§€ ì‹œ ì™„ì „ ì°¨ë‹¨
        
        # 5. ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(cleaned.strip()) < 1:
            return ""
            
        # 6. ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì²´í¬ (ì™„í™”ëœ ë²„ì „)
        meaningful_chars = sum(1 for c in cleaned if c.isalnum() or c in '()[]{}.,;:"\'\_-+=<>!?#$%&@^`~|\\ \t\n')
        total_chars = len(cleaned)
        
        # ë„ˆë¬´ ì§§ì€ ë¸íƒ€ëŠ” í†µê³¼
        if total_chars <= 3:
            return cleaned
        
        # ë¹„ìœ¨ ê³„ì‚° (ê³µë°± ë¬¸ì í¬í•¨)
        if total_chars > 0:
            ratio = meaningful_chars / total_chars
            # 10% ì´ìƒì´ë©´ í†µê³¼ (ê¸°ì¡´ 30%ì—ì„œ ì™„í™”)
            if ratio < 0.1:
                logger.warning(f"ì˜ë¯¸ ì—†ëŠ” ë¸íƒ€ ì°¨ë‹¨: meaningful_ratio={meaningful_chars}/{total_chars} = {ratio:.2f}")
                return ""
            else:
                logger.debug(f"ë¸íƒ€ í†µê³¼: meaningful_ratio={meaningful_chars}/{total_chars} = {ratio:.2f}")
        
        return cleaned