# app/engine.py
"""
vLLM ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  í…ìŠ¤íŠ¸ ìƒì„± ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì½”ì–´ ëª¨ë“ˆ.
VRAM ìµœì í™”, ë²ˆì—­ ì„œë¹„ìŠ¤ ì—°ë™, ìƒì„± ì¤‘ë‹¨ ë¡œì§ì´ ëª¨ë‘ í¬í•¨ëœ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.
"""
import logging
import time
import json
from typing import Dict, List, Any, AsyncGenerator
import httpx  # API í˜¸ì¶œì„ ìœ„í•œ httpx

from fastapi import HTTPException
from transformers import AutoTokenizer
from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.lora.request import LoRARequest

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from .config import ModelConfig
from .schemas import GenerateRequest, StreamGenerateRequest
# âœ¨ prompt_builder.pyë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
# from .prompt_builder import generate_enhanced_prompt

logger = logging.getLogger(__name__)

# ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ ì£¼ì†Œ (docker-compose ì„œë¹„ìŠ¤ ì´ë¦„ ê¸°ì¤€)
TRANSLATOR_API_URL = "http://translator:8003/translate"

async def call_translation_service(korean_prompt: str) -> str:
    """ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œì‘: '{korean_prompt}'")
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(TRANSLATOR_API_URL, json={"text": korean_prompt})
            response.raise_for_status()
            data = response.json()
            translated_text = data.get('translated_text', korean_prompt)
            logger.info(f"ë²ˆì—­ ì™„ë£Œ: -> '{translated_text}'")
            return translated_text
    except httpx.RequestError as e:
        logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤({e.request.url}) í˜¸ì¶œ ì‹¤íŒ¨: {e}. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return korean_prompt

class VLLMMultiLoRAEngine:
    """vLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—”ì§„ í´ë˜ìŠ¤"""

    def __init__(self, base_model_path: str, model_configs: Dict[str, ModelConfig]):
        self.base_model_path = base_model_path
        self.model_configs = model_configs
        self.engine = None
        self.tokenizer = None
        self.is_initialized = False

    def initialize_engine(self):
        """vLLM ë¹„ë™ê¸° ì—”ì§„ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ vLLM ë¹„ë™ê¸° ì—”ì§„ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘ (VRAM ìµœì í™” ëª¨ë“œ)...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            
            # âœ¨ VRAM ìµœì í™” ë° ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ìµœì¢… EngineArgs ì„¤ì •
            engine_args = AsyncEngineArgs(
                model=self.base_model_path,
                quantization="marlin",
                enable_lora=True,
                trust_remote_code=True,
                gpu_memory_utilization=0.4,
                max_model_len=2048,
                max_num_seqs=4,
                tokenizer_mode="slow",
                max_loras=5,
                max_lora_rank=32,
                dtype="half",
                tensor_parallel_size=1,
            )
            
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            self.is_initialized = True
            logger.info("âœ… ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (VRAM ìµœì í™” ëª¨ë“œ)")
        except Exception as e:
            logger.error(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self.is_initialized = False
            raise

    def _build_chatml_prompt(self, config: ModelConfig, user_prompt: str, model_type: str) -> str:
        """ChatML í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        
        # FIM íƒœê·¸ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬
        is_fim_request = "<ï½œfim beginï½œ>" in user_prompt or "<|fim_begin|>" in user_prompt
        
        if is_fim_request:
            logger.info("FIM ìš”ì²­ ê°ì§€ë¨ - ì½”ë“œ ì™„ì„± ëª¨ë“œ í™œì„±í™”")
            # FIM ìš”ì²­ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
            user_prompt = self._enhance_fim_prompt(user_prompt)
        
        # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if model_type == "autocomplete":
            system_content = f"""
You are an expert Python code completion assistant.

Your task:
- Complete the user's code snippet by filling in the missing parts
- Provide only the necessary code to complete the functionality
- Use proper Python syntax and best practices
- Keep completions concise and relevant

Output format:
- Provide ONLY the completed code
- Do NOT add explanations or comments unless specifically requested
- Always end with: # --- Generation Complete ---

Remember: You are completing code, not writing essays.
""".strip()
        
        elif model_type == "prompt":
            system_content = f"""
You are a helpful senior Python developer with expertise in writing clean, efficient code.

Your task:
- Generate Python code based on the user's request
- Provide working, tested code solutions
- Follow Python best practices and PEP 8 guidelines
- Include necessary imports and error handling

Output format:
1. First, provide the complete code solution
2. Then, on a new line, write '--- Rationale ---' followed by a brief explanation
3. Always end with: # --- Generation Complete ---

Remember: Focus on practical, working solutions.
""".strip()
        
        elif model_type == "comment":
            system_content = f"""
You are a Python code documentation specialist.

Your task:
- Generate clear, concise comments and docstrings
- Explain code functionality and purpose
- Follow Python documentation standards
- Use proper docstring formats (Google/NumPy style)

Output format:
- Provide the requested comments/docstrings
- Keep explanations clear and technical
- Always end with: # --- Generation Complete ---

Remember: Documentation should be helpful and accurate.
""".strip()
        
        elif model_type == "error_fix":
            system_content = f"""
You are a Python debugging expert specializing in error analysis and fixes.

Your task:
- Analyze the provided code and identify issues
- Provide corrected code with fixes applied
- Explain what was wrong and how it was fixed
- Ensure the solution handles edge cases

Output format:
1. First, provide the corrected code
2. Then, write '--- Fix Explanation ---' followed by what was wrong and how you fixed it
3. Always end with: # --- Generation Complete ---

Remember: Focus on robust, error-free solutions.
""".strip()
        
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_content = config.system_prompt
        
        # í›ˆë ¨ ë°ì´í„° í˜•ì‹ì— ë§ëŠ” ChatML í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œì™¸)
        # í›ˆë ¨ ë°ì´í„°ì—ì„œëŠ” user â†’ assistant ì§ì ‘ ëŒ€í™” í˜•ì‹ì„ ì‚¬ìš©í–ˆìŒ
        chatml_prompt = f"""<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""
        
        logger.debug(f"ChatML í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ (ëª¨ë¸: {model_type}) - í›ˆë ¨ ë°ì´í„° í˜•ì‹ í˜¸í™˜")
        return chatml_prompt

    def _enhance_fim_prompt(self, user_prompt: str) -> str:
        """FIM ìš”ì²­ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
        # FIM íƒœê·¸ë¥¼ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¡œ ë³€í™˜
        enhanced_prompt = user_prompt
        
        # ë‹¤ì–‘í•œ FIM íƒœê·¸ í˜•ì‹ ì§€ì›
        fim_patterns = [
            ("<ï½œfim beginï½œ>", "[CODE_START]"),
            ("<ï½œfim holeï½œ>", "[FILL_HERE]"),
            ("<ï½œfim endï½œ>", "[CODE_END]"),
            ("<|fim_begin|>", "[CODE_START]"),
            ("<|fim_hole|>", "[FILL_HERE]"),
            ("<|fim_end|>", "[CODE_END]")
        ]
        
        for old_tag, new_tag in fim_patterns:
            enhanced_prompt = enhanced_prompt.replace(old_tag, new_tag)
        
        # FIM ìš”ì²­ì— ëŒ€í•œ ìì—°ì–´ ì„¤ëª… ì¶”ê°€ (í›ˆë ¨ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ í•œêµ­ì–´ ì‚¬ìš©)
        if "[FILL_HERE]" in enhanced_prompt:
            enhanced_prompt = f"""ì£¼ì„ì— ë”°ë¼ ì ì ˆí•œ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì´ì „ ì½”ë“œ:
{enhanced_prompt.replace('[FILL_HERE]', '// ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‚½ì…í•´ì•¼ í•¨ //')}

ìœ„ ì½”ë“œì—ì„œ ë¹„ì–´ìˆëŠ” ë¶€ë¶„ì„ ì±„ì›Œì£¼ì„¸ìš”. Python ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¥´ê³  ì£¼ë³€ ì½”ë“œì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ í•´ì£¼ì„¸ìš”."""
        
        return enhanced_prompt

    def _get_stop_token_ids(self, stop_strings: List[str]) -> List[int]:
        """ìŠ¤íƒ‘ ë¬¸ìì—´ì„ í† í° IDë¡œ ë³€í™˜"""
        stop_token_ids = []
        
        if self.tokenizer is None:
            return stop_token_ids
            
        # EOS í† í° ë° íŠ¹ìˆ˜ í† í° ê°•ì œ ì¶”ê°€
        special_tokens = [
            self.tokenizer.eos_token_id,  # EOS í† í°
            self.tokenizer.pad_token_id,  # PAD í† í°
        ]
        
        for token_id in special_tokens:
            if token_id is not None:
                stop_token_ids.append(token_id)
            
        for stop_str in stop_strings:
            try:
                # ë¬¸ìì—´ì„ í† í° IDë¡œ ë³€í™˜
                token_ids = self.tokenizer.encode(stop_str, add_special_tokens=False)
                stop_token_ids.extend(token_ids)
                
                # ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš© (ë” ì •í™•í•œ ë§¤ì¹­)
                if token_ids:
                    stop_token_ids.append(token_ids[-1])
                    
            except Exception as e:
                logger.warning(f"ìŠ¤íƒ‘ í† í° '{stop_str}' ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        stop_token_ids = list(set(stop_token_ids))
        logger.info(f"ìŠ¤íƒ‘ í† í° ID: {stop_token_ids}")
        
        return stop_token_ids

    async def generate_stream(self, request: StreamGenerateRequest) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_initialized or self.engine is None or self.tokenizer is None:
            raise HTTPException(status_code=503, detail="Model engine not initialized")

        if request.model_type not in self.model_configs:
            error_msg = f"Model type '{request.model_type}' not configured."
            logger.error(error_msg)
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
            return
            
        english_prompt = await call_translation_service(request.prompt)

        config = self.model_configs[request.model_type]
        
        # âœ¨ ChatML í˜•ì‹ìœ¼ë¡œ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìŠ¤íƒ‘ í† í° ì¸ì‹ ê°œì„ )
        final_prompt = self._build_chatml_prompt(config, english_prompt, request.model_type)

        # í† í° ID ê¸°ë°˜ ìŠ¤íƒ‘ í† í° ê³„ì‚°
        stop_token_ids = self._get_stop_token_ids(config.stop or [])
        
        sampling_params = SamplingParams(
            temperature=request.temperature if request.temperature is not None else config.temperature,
            top_p=request.top_p if request.top_p is not None else config.top_p,
            max_tokens=request.max_tokens,
            stop=config.stop or [],
            stop_token_ids=stop_token_ids,  # í† í° ID ê¸°ë°˜ ìŠ¤íƒ‘
            repetition_penalty=1.3,  # ë°˜ë³µ ë°©ì§€ ë” ê°•í™”
            frequency_penalty=0.2,   # ë¹ˆë„ ê¸°ë°˜ í˜ë„í‹° ì¦ê°€
            presence_penalty=0.2,    # ì¡´ì¬ ê¸°ë°˜ í˜ë„í‹° ì¦ê°€
            include_stop_str_in_output=False,  # stop í† í°ì„ ì¶œë ¥ì—ì„œ ì œì™¸
            skip_special_tokens=False  # íŠ¹ìˆ˜ í† í° ìœ ì§€
        )
        
        # FIM íƒœê·¸ ì¸ì‹ì„ ìœ„í•œ ë¡œê¹…
        logger.info(f"Stop tokens for {request.model_type}: {config.stop}")

        lora_request = LoRARequest(lora_name=config.name, lora_int_id=config.lora_id, lora_path=config.adapter_path)
        request_id = f"stream-{int(time.time() * 1000)}"
        results_generator = self.engine.generate(final_prompt, sampling_params, request_id, lora_request)

        # âœ¨ ê°•í™”ëœ ìŠ¤íƒ‘ í† í° ê°ì§€ ë¡œì§
        last_text = ""
        buffer = ""  # ë¶€ë¶„ ìŠ¤íƒ‘ í† í° ê°ì§€ë¥¼ ìœ„í•œ ë²„í¼
        
        async for request_output in results_generator:
            text_so_far = request_output.outputs[0].text
            buffer += text_so_far[len(last_text):]
            
            # 1. ì™„ì „í•œ ìŠ¤íƒ‘ í† í° ì²´í¬
            stop_found = False
            stop_str_found = ""
            for stop_str in (config.stop or []):
                if stop_str in text_so_far:
                    text_so_far = text_so_far.split(stop_str)[0]
                    stop_found = True
                    stop_str_found = stop_str
                    logger.info(f"ì™„ì „í•œ ìŠ¤íƒ‘ í† í° '{stop_str_found}' ê°ì§€ë¨")
                    break
            
            # 2. ë¶€ë¶„ ìŠ¤íƒ‘ í† í° ì²´í¬ (ê°„ì†Œí™”)
            # ë¶€ë¶„ ìŠ¤íƒ‘ í† í° ê°ì§€ ë¹„í™œì„±í™” - ë‹¨ìˆœí•˜ê²Œ ì™„ì „í•œ ìŠ¤íƒ‘ í† í°ë§Œ ì²˜ë¦¬
            
            # 3. ë¸íƒ€ ê³„ì‚° ë° ì „ì†¡ (ê°„ì†Œí™”)
            delta = text_so_far[len(last_text):]
            if delta and not stop_found:
                # ë¸íƒ€ í•„í„°ë§ ë¹„í™œì„±í™” - ëª¨ë“  ë¸íƒ€ ì „ì†¡
                yield f"data: {json.dumps({'text': delta})}\n\n"
            
            last_text = text_so_far
            
            # 4. ìŠ¤íƒ‘ ì¡°ê±´ ì²´í¬
            if stop_found:
                logger.info(f"ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ: '{stop_str_found}' ê°ì§€")
                break
                
            # 5. ë‹¤ì–‘í•œ ì¢…ë£Œ ì¡°ê±´ ì²´í¬
            # 5-1. ìµœëŒ€ í† í° ìˆ˜ ì²´í¬
            if len(text_so_far.split()) >= request.max_tokens:
                logger.info(f"ìµœëŒ€ í† í° ìˆ˜({request.max_tokens}) ë„ë‹¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                break
                
            # 5-2. ë¬¸ì ìˆ˜ ê¸°ë°˜ ì¢…ë£Œ (ëª¨ë¸ íƒ€ì…ë³„) - ì œí•œ ì™„í™”
            max_chars = {
                "autocomplete": 1000,
                "prompt": 4000, 
                "comment": 1500,
                "error_fix": 3000
            }.get(request.model_type, 2000)
            
            if len(text_so_far) >= max_chars:
                logger.info(f"ìµœëŒ€ ë¬¸ì ìˆ˜({max_chars}) ë„ë‹¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                break
                
            # 5-3. ì—°ì† ë™ì¼ ë‚´ìš© ë°˜ë³µ ê°ì§€ (ì™„í™”)
            if len(last_text) > 100:  # ë” ê¸´ í…ìŠ¤íŠ¸ì—ì„œë§Œ ì²´í¬
                recent_text = last_text[-100:]  # ë” ê¸´ íŒ¨í„´ ì²´í¬
                if recent_text in text_so_far[:-100] and len(text_so_far) > 300:  # ë” ì—„ê²©í•œ ì¡°ê±´
                    logger.info("ë°˜ë³µ ë‚´ìš© ê°ì§€ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                    break
                    
            # 5-4. íŒ¨í„´ ê¸°ë°˜ ê°•ì œ ì¢…ë£Œ (ë¹„í™œì„±í™”)
            # explanation_patterns = [
            #     "\n\nThis code", "\n\nThe above", "\n\nHere's", "\n\nLet me",
            #     "\n\nExplanation:", "\n\nNote:", "\n\nRemember", "\n\nI hope",
            #     "\n\nì´ ì½”ë“œ", "\n\nìœ„ ì½”ë“œ", "\n\nì„¤ëª…:", "\n\nì°¸ê³ :"
            # ]
            # 
            # for pattern in explanation_patterns:
            #     if pattern.lower() in text_so_far.lower():
            #         # ì„¤ëª… ì‹œì‘ ì „ê¹Œì§€ë§Œ ìë¥´ê¸°
            #         cut_index = text_so_far.lower().find(pattern.lower())
            #         if cut_index > 0:
            #             text_so_far = text_so_far[:cut_index].strip()
            #             logger.info(f"ì„¤ëª… íŒ¨í„´ '{pattern}' ê°ì§€ - ì½”ë“œë¶€ë¶„ë§Œ ìœ ì§€")
            #             # ë§ˆì§€ë§‰ ë¸íƒ€ ì „ì†¡ í›„ ì¢…ë£Œ
            #             final_delta = text_so_far[len(last_text):]
            #             if final_delta:
            #                 yield f"data: {json.dumps({'text': final_delta})}\n\n"
            #             # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
            #             yield f"data: {json.dumps({'type': 'done', 'text': ''})}\n\n"
            #             return

        # ì •ìƒ ì¢…ë£Œ ì‹œ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
        yield f"data: {json.dumps({'type': 'done', 'text': ''})}\n\n"

    async def generate_single(self, request: GenerateRequest) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        config = self.model_configs[request.model_type]
        stream_request = StreamGenerateRequest(
            user_id=0,
            prompt=request.prompt,
            model_type=request.model_type,
            user_select_options=request.user_select_options,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens if request.max_tokens is not None else config.max_tokens,
        )

        full_text = ""
        async for chunk in self.generate_stream(stream_request):
            if chunk.strip() == "data: [DONE]":
                break
            try:
                data_str = chunk.strip()[5:].strip()
                if data_str:
                    data = json.loads(data_str)
                    if "text" in data:
                        full_text += data["text"]
                    elif "error" in data:
                        logger.error(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ ì˜¤ë¥˜ ìˆ˜ì‹ : {data['error']}")
                        return {'error': data['error']}
            except json.JSONDecodeError:
                logger.warning(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ JSON ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ: {chunk}")
                continue
        return {'generated_text': full_text}