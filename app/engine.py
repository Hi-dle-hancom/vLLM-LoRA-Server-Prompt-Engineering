# app/engine.py
"""
vLLM ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  í…ìŠ¤íŠ¸ ìƒì„± ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì½”ì–´ ëª¨ë“ˆ.
VRAM ìµœì í™”, ë²ˆì—­ ì„œë¹„ìŠ¤ ì—°ë™, ìƒì„± ì¤‘ë‹¨ ë¡œì§ì´ ëª¨ë‘ í¬í•¨ëœ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.
"""
import logging
import time
import json
from typing import Dict, List, Any, AsyncGenerator
import httpx  # API í˜¸ì¶œì„ ìœ„í•œ httpx

from fastapi import HTTPException
from transformers import AutoTokenizer
from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.lora.request import LoRARequest

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from .config import ModelConfig
from .schemas import GenerateRequest, StreamGenerateRequest
# âœ¨ prompt_builder.pyë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
# from .prompt_builder import generate_enhanced_prompt

logger = logging.getLogger(__name__)

# ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ ì£¼ì†Œ (docker-compose ì„œë¹„ìŠ¤ ì´ë¦„ ê¸°ì¤€)
TRANSLATOR_API_URL = "http://translator:8003/translate"

async def call_translation_service(korean_prompt: str) -> str:
    """ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œì‘: '{korean_prompt}'")
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(TRANSLATOR_API_URL, json={"text": korean_prompt})
            response.raise_for_status()
            data = response.json()
            translated_text = data.get('translated_text', korean_prompt)
            logger.info(f"ë²ˆì—­ ì™„ë£Œ: -> '{translated_text}'")
            return translated_text
    except httpx.RequestError as e:
        logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤({e.request.url}) í˜¸ì¶œ ì‹¤íŒ¨: {e}. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return korean_prompt

class VLLMMultiLoRAEngine:
    """vLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—”ì§„ í´ë˜ìŠ¤"""

    def __init__(self, base_model_path: str, model_configs: Dict[str, ModelConfig]):
        self.base_model_path = base_model_path
        self.model_configs = model_configs
        self.engine = None
        self.tokenizer = None
        self.is_initialized = False

    def initialize_engine(self):
        """vLLM ë¹„ë™ê¸° ì—”ì§„ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ vLLM ë¹„ë™ê¸° ì—”ì§„ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘ (VRAM ìµœì í™” ëª¨ë“œ)...")
        try:
            # ğŸ›¡ï¸ ì•ˆì „í•œ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ë°”ì´íŠ¸ ê²½ê³„ ì˜¤ë¥˜ ë°©ì§€)
            logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘: {self.base_model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_path,
                trust_remote_code=True,
                use_fast=False,  # Fast í† í¬ë‚˜ì´ì € ë¹„í™œì„±í™” (ì•ˆì •ì„± ìš°ì„ )
                padding_side="left",  # íŒ¨ë”© ë°©í–¥ ëª…ì‹œ
                truncation_side="left",  # ì˜ë¦¼ ë°©í–¥ ëª…ì‹œ
                clean_up_tokenization_spaces=True,  # í† í°í™” ê³µë°± ì •ë¦¬
            )
            
            # ğŸ”§ í† í¬ë‚˜ì´ì € ì•ˆì „ì„± ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                logger.info("pad_tokenì„ eos_tokenìœ¼ë¡œ ì„¤ì •")
            
            if self.tokenizer.unk_token is None:
                self.tokenizer.unk_token = "<unk>"
                logger.info("unk_token ì„¤ì •")
                
            logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ: vocab_size={self.tokenizer.vocab_size}")
            
            # âœ¨ ê·¹ë„ë¡œ ë³´ìˆ˜ì ì¸ ì•ˆì „ ì„¤ì • (í…ìŠ¤íŠ¸ ì†ìƒ ë°©ì§€)
            engine_args = AsyncEngineArgs(
                model=self.base_model_path,
                # quantization ëª…ì‹œì  ë¹„í™œì„±í™” (í† í° ì†ìƒ ë°©ì§€)
                quantization=None,  # ëª…ì‹œì  None ì„¤ì •
                enable_lora=True,
                trust_remote_code=True,
                gpu_memory_utilization=0.4,  # ë¹„ì–‘ìí™” ëª¨ë¸ì— ë§ê²Œ ê°ì†Œ
                max_model_len=1536,  # ë¹„ì–‘ìí™” ëª¨ë¸ì— ë§ê²Œ ê°ì†Œ
                max_num_seqs=1,  # ë‹¨ì¼ ì‹œí€€ìŠ¤ ìœ ì§€
                tokenizer_mode="slow",  # auto â†’ slow (ë°”ì´íŠ¸ ê²½ê³„ ì˜¤ë¥˜ ë°©ì§€)
                max_loras=2,  # 4 â†’ 2 ìµœì†Œí™” (ë©”ëª¨ë¦¬ ì•ˆì •ì„±)
                max_lora_rank=16,  # 8 â†’ 16 ìˆ˜ì • (ì‹¤ì œ ëª¨ë¸ ëœí¬ì™€ ì¼ì¹˜)
                dtype="half",  # float32 â†’ half ë³µì› (í˜¸í™˜ì„± ìš°ì„ )
                tensor_parallel_size=1,
                # ìµœëŒ€ ì•ˆì •ì„± ì˜µì…˜
                enforce_eager=True,  # CUDA ê·¸ë˜í”„ ì™„ì „ ë¹„í™œì„±í™”
                disable_custom_all_reduce=True,  # ì»¤ìŠ¤í…€ reduce ë¹„í™œì„±í™”
                swap_space=4,  # ìŠ¤ì™ˆ ê³µê°„ ì¶”ê°€ (ë©”ëª¨ë¦¬ ì•ˆì •ì„±)
                # ëª¨ë“  ìµœì í™” ë¹„í™œì„±í™”
            )
            
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            self.is_initialized = True
            logger.info("âœ… ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (VRAM ìµœì í™” ëª¨ë“œ)")
            
            # ëª¨ë¸ ìƒíƒœ ê°„ë‹¨ ê²€ì¦
            logger.info("ğŸ” ëª¨ë¸ ìƒíƒœ ê²€ì¦ ì‹œì‘...")
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
                test_prompt = "Hello"
                test_params = SamplingParams(temperature=0.1, max_tokens=5)
                # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ëŠ” ì—¬ê¸°ì„œ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ (ì´ˆê¸°í™” ë‹¨ê³„ì—ì„œëŠ” ìœ„í—˜)
                logger.info("âœ… ëª¨ë¸ ê¸°ë³¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âš ï¸ ëª¨ë¸ ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.error(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self.is_initialized = False
            raise

    def _build_chatml_prompt(self, config: ModelConfig, user_prompt: str, model_type: str) -> str:
        """ChatML ë˜ëŠ” FIM í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤ (í•™ìŠµ ë°ì´í„° í˜•ì‹ ì¤€ìˆ˜)."""
        
        # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ê²°ì •
        is_fim_model = model_type in ["autocomplete", "comment"]
        is_fim_request = "<ï½œfim beginï½œ>" in user_prompt or "<|fim_begin|>" in user_prompt
        
        # FIM ëª¨ë¸ì´ê±°ë‚˜ FIM ìš”ì²­ì¸ ê²½ìš° FIM í˜•ì‹ ì‚¬ìš©
        if is_fim_model or is_fim_request:
            logger.info(f"FIM í˜•ì‹ ì‚¬ìš© - ëª¨ë¸: {model_type}, FIM ìš”ì²­: {is_fim_request}")
            return self._build_fim_prompt(user_prompt, model_type)
        
        # ChatML ëª¨ë¸ì€ ChatML í˜•ì‹ ì‚¬ìš©
        logger.info(f"ChatML í˜•ì‹ ì‚¬ìš© - ëª¨ë¸: {model_type}")
        return self._build_chatML_format(config, user_prompt, model_type)
    
    def _build_fim_prompt(self, user_prompt: str, model_type: str) -> str:
        """FIM (Fill-in-Middle) í˜•ì‹ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìë™ì™„ì„±/ì£¼ì„ ì „ìš©)."""
        
        # FIM íƒœê·¸ê°€ ì´ë¯¸ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if "<|fim_begin|>" in user_prompt or "<ï½œfim beginï½œ>" in user_prompt:
            enhanced_prompt = self._enhance_fim_prompt(user_prompt)
            return enhanced_prompt
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ FIM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if model_type == "autocomplete":
            # ìë™ì™„ì„±: ì½”ë“œ ë’¤ì— ì»¤ì„œ ìœ„ì¹˜ ì„¤ì •
            fim_prompt = f"<|fim_begin|>{user_prompt}<|fim_hole|><|fim_end|>"
        elif model_type == "comment":
            # ì£¼ì„: ì½”ë“œ ìœ„ì— ì£¼ì„ ì‚½ì… ìœ„ì¹˜ ì„¤ì •
            fim_prompt = f"<|fim_begin|><|fim_hole|>\n{user_prompt}<|fim_end|>"
        else:
            # ê¸°ë³¸: ì¼ë°˜ FIM í˜•ì‹
            fim_prompt = f"<|fim_begin|>{user_prompt}<|fim_hole|><|fim_end|>"
        
        logger.debug(f"FIM í”„ë¡¬í”„íŠ¸ ìƒì„±: {fim_prompt[:100]}...")
        return fim_prompt
    
    def _build_chatML_format(self, config: ModelConfig, user_prompt: str, model_type: str) -> str:
        """ChatML í˜•ì‹ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (prompt, error_fix ì „ìš©)."""
        
        # FIM íƒœê·¸ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        is_fim_request = "<ï½œfim beginï½œ>" in user_prompt or "<|fim_begin|>" in user_prompt
        
        if is_fim_request:
            logger.info("FIM ìš”ì²­ ê°ì§€ë¨ - ì½”ë“œ ì™„ì„± ëª¨ë“œ í™œì„±í™”")
            # FIM ìš”ì²­ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
            user_prompt = self._enhance_fim_prompt(user_prompt)
        
        # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if model_type == "autocomplete":
            system_content = f"""
You are an expert Python code completion assistant.

Your task:
- Complete the user's code snippet by filling in the missing parts
- Provide only the necessary code to complete the functionality
- Use proper Python syntax and best practices
- Keep completions concise and relevant

Output format:
- Provide ONLY the completed code
- Do NOT add explanations or comments unless specifically requested
- Always end with: # --- Generation Complete ---

Remember: You are completing code, not writing essays.
""".strip()
        
        elif model_type == "prompt":
            system_content = f"""
You are a helpful senior Python developer with expertise in writing clean, efficient code.

Your task:
- Generate Python code based on the user's request
- Provide working, tested code solutions
- Follow Python best practices and PEP 8 guidelines
- Include necessary imports and error handling

Output format:
1. First, provide the complete code solution
2. Then, on a new line, write '--- Rationale ---' followed by a brief explanation
3. Always end with: # --- Generation Complete ---

Remember: Focus on practical, working solutions.
""".strip()
        
        elif model_type == "comment":
            system_content = f"""
You are a Python code documentation specialist.

Your task:
- Generate clear, concise comments and docstrings
- Explain code functionality and purpose
- Follow Python documentation standards
- Use proper docstring formats (Google/NumPy style)

Output format:
- Provide the requested comments/docstrings
- Keep explanations clear and technical
- Always end with: # --- Generation Complete ---

Remember: Documentation should be helpful and accurate.
""".strip()
        
        elif model_type == "error_fix":
            system_content = f"""
You are a Python debugging expert specializing in error analysis and fixes.

Your task:
- Analyze the provided code and identify issues
- Provide corrected code with fixes applied
- Explain what was wrong and how it was fixed
- Ensure the solution handles edge cases

Output format:
1. First, provide the corrected code
2. Then, write '--- Fix Explanation ---' followed by what was wrong and how you fixed it
3. Always end with: # --- Generation Complete ---

Remember: Focus on robust, error-free solutions.
""".strip()
        
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_content = config.system_prompt
        
        # ChatML í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
        chatml_prompt = f"""<|im_start|>system
{system_content}<|im_end|>
<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""
        
        logger.debug(f"ChatML í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ (ëª¨ë¸: {model_type}) - í›ˆë ¨ ë°ì´í„° í˜•ì‹ í˜¸í™˜")
        return chatml_prompt

    def _enhance_fim_prompt(self, user_prompt: str) -> str:
        """FIM ìš”ì²­ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
        # FIM íƒœê·¸ë¥¼ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¡œ ë³€í™˜
        enhanced_prompt = user_prompt
        
        # ë‹¤ì–‘í•œ FIM íƒœê·¸ í˜•ì‹ ì§€ì›
        fim_patterns = [
            ("<ï½œfim beginï½œ>", "[CODE_START]"),
            ("<ï½œfim holeï½œ>", "[FILL_HERE]"),
            ("<ï½œfim endï½œ>", "[CODE_END]"),
            ("<|fim_begin|>", "[CODE_START]"),
            ("<|fim_hole|>", "[FILL_HERE]"),
            ("<|fim_end|>", "[CODE_END]")
        ]
        
        for old_tag, new_tag in fim_patterns:
            enhanced_prompt = enhanced_prompt.replace(old_tag, new_tag)
        
        # FIM ìš”ì²­ì— ëŒ€í•œ ìì—°ì–´ ì„¤ëª… ì¶”ê°€ (í›ˆë ¨ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ í•œêµ­ì–´ ì‚¬ìš©)
        if "[FILL_HERE]" in enhanced_prompt:
            enhanced_prompt = f"""ì£¼ì„ì— ë”°ë¼ ì ì ˆí•œ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì´ì „ ì½”ë“œ:
{enhanced_prompt.replace('[FILL_HERE]', '// ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‚½ì…í•´ì•¼ í•¨ //')}

ìœ„ ì½”ë“œì—ì„œ ë¹„ì–´ìˆëŠ” ë¶€ë¶„ì„ ì±„ì›Œì£¼ì„¸ìš”. Python ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë”°ë¥´ê³  ì£¼ë³€ ì½”ë“œì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ í•´ì£¼ì„¸ìš”."""
        
        return enhanced_prompt

    def _get_stop_token_ids(self, stop_strings: List[str]) -> List[int]:
        """ìŠ¤íƒ‘ ë¬¸ìì—´ì„ í† í° IDë¡œ ë³€í™˜"""
        stop_token_ids = []
        
        if self.tokenizer is None:
            return stop_token_ids
            
        # EOS í† í° ë° íŠ¹ìˆ˜ í† í° ê°•ì œ ì¶”ê°€
        special_tokens = [
            self.tokenizer.eos_token_id,  # EOS í† í°
            self.tokenizer.pad_token_id,  # PAD í† í°
        ]
        
        for token_id in special_tokens:
            if token_id is not None:
                stop_token_ids.append(token_id)
            
        for stop_str in stop_strings:
            try:
                # ë¬¸ìì—´ì„ í† í° IDë¡œ ë³€í™˜
                token_ids = self.tokenizer.encode(stop_str, add_special_tokens=False)
                stop_token_ids.extend(token_ids)
                
                # ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš© (ë” ì •í™•í•œ ë§¤ì¹­)
                if token_ids:
                    stop_token_ids.append(token_ids[-1])
                    
            except Exception as e:
                logger.warning(f"ìŠ¤íƒ‘ í† í° '{stop_str}' ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        stop_token_ids = list(set(stop_token_ids))
        logger.info(f"ìŠ¤íƒ‘ í† í° ID: {stop_token_ids}")
        
        return stop_token_ids

    async def generate_stream(self, request: StreamGenerateRequest) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_initialized or self.engine is None or self.tokenizer is None:
            raise HTTPException(status_code=503, detail="Model engine not initialized")

        if request.model_type not in self.model_configs:
            error_msg = f"Model type '{request.model_type}' not configured."
            logger.error(error_msg)
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
            return
            
        english_prompt = await call_translation_service(request.prompt)

        config = self.model_configs[request.model_type]
        
        # âœ¨ ChatML í˜•ì‹ìœ¼ë¡œ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìŠ¤íƒ‘ í† í° ì¸ì‹ ê°œì„ )
        final_prompt = self._build_chatml_prompt(config, english_prompt, request.model_type)

        # í† í° ID ê¸°ë°˜ ìŠ¤íƒ‘ í† í° ê³„ì‚°
        stop_token_ids = self._get_stop_token_ids(config.stop or [])
        
        # ğŸ›¡ï¸ ê·¹ë„ë¡œ ì•ˆì „í•œ SamplingParams (ë©”ëª¨ë¦¬ ì†ìƒ ë°©ì§€)
        safe_temperature = max(0.1, min(1.0, request.temperature if request.temperature is not None else config.temperature))
        safe_top_p = max(0.1, min(1.0, request.top_p if request.top_p is not None else config.top_p))
        safe_max_tokens = min(512, max(1, request.max_tokens))  # í† í° ìˆ˜ ì œí•œ
        
        sampling_params = SamplingParams(
            temperature=safe_temperature,
            top_p=safe_top_p,
            max_tokens=safe_max_tokens,
            stop=config.stop or [],
            stop_token_ids=stop_token_ids,
            repetition_penalty=1.1,  # 1.3 â†’ 1.1 ì™„í™” (ì•ˆì •ì„±)
            frequency_penalty=0.1,   # 0.2 â†’ 0.1 ì™„í™” (ì•ˆì •ì„±)
            presence_penalty=0.1,    # 0.2 â†’ 0.1 ì™„í™” (ì•ˆì •ì„±)
            include_stop_str_in_output=False,
            skip_special_tokens=True,  # False â†’ True (íŠ¹ìˆ˜ í† í° ì œê±°ë¡œ ì•ˆì •ì„±)
            # ì¶”ê°€ ì•ˆì „ì„± ì˜µì…˜
            logprobs=None,  # logprobs ë¹„í™œì„±í™”
            prompt_logprobs=None,  # prompt logprobs ë¹„í™œì„±í™”
            detokenize=True,  # ë””í† í°í™” í™œì„±í™”
            spaces_between_special_tokens=True,  # íŠ¹ìˆ˜ í† í° ê°„ ê³µë°±
        )
        
        logger.info(f"ì•ˆì „í•œ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°: temp={safe_temperature}, top_p={safe_top_p}, max_tokens={safe_max_tokens}")
        
        # FIM íƒœê·¸ ì¸ì‹ì„ ìœ„í•œ ë¡œê¹…
        logger.info(f"Stop tokens for {request.model_type}: {config.stop or []}")
        
        # request_id ìƒì„± (ì‚¬ìš© ì „ì— ë¨¼ì € ì •ì˜)
        request_id = f"stream-{int(time.time() * 1000)}"
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘
        logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘: request_id={request_id}")
        last_text = ""
        token_count = 0
        lora_request = LoRARequest(lora_name=config.name, lora_int_id=config.lora_id, lora_path=config.adapter_path)
        results_generator = self.engine.generate(final_prompt, sampling_params, request_id, lora_request)

        # ğŸ›¡ï¸ ê·¹ë„ë¡œ ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ (ë©”ëª¨ë¦¬ ì†ìƒ ë°©ì§€)
        max_iterations = 1000  # ë¬´í•œ ë£¨í”„ ë°©ì§€
        iteration_count = 0
        
        try:
            async for request_output in results_generator:
                iteration_count += 1
                if iteration_count > max_iterations:
                    logger.error(f"ìµœëŒ€ ë°˜ë³µ ìˆ˜({max_iterations}) ì´ˆê³¼ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                    break
                
                # ì•ˆì „í•œ ì¶œë ¥ ì¶”ì¶œ
                try:
                    if not request_output.outputs or len(request_output.outputs) == 0:
                        logger.warning("ë¹ˆ ì¶œë ¥ ê°ì§€, ê±´ë„ˆë›°ê¸°")
                        continue
                        
                    current_text = request_output.outputs[0].text
                    if not isinstance(current_text, str):
                        logger.error(f"ë¹„ë¬¸ìì—´ ì¶œë ¥ ê°ì§€: {type(current_text)}")
                        continue
                        
                except (IndexError, AttributeError) as e:
                    logger.error(f"ì¶œë ¥ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                    continue
                
                token_count += 1
                
                # 1. ìŠ¤í†± í† í° ì²´í¬
                stop_found = False
                try:
                    for stop_str in (config.stop or []):
                        if stop_str and stop_str in current_text:
                            current_text = current_text.split(stop_str)[0]
                            stop_found = True
                            logger.info(f"ìŠ¤í†± í† í° '{stop_str}' ê°ì§€ë¨")
                            break
                except Exception as e:
                    logger.error(f"ìŠ¤í†± í† í° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                # 2. ì•ˆì „í•œ ë¸íƒ€ ê³„ì‚°
                try:
                    if len(current_text) >= len(last_text):
                        delta = current_text[len(last_text):]
                    else:
                        logger.warning("í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ì´ì „ë³´ë‹¤ ì§§ìŒ, ë¸íƒ€ ê±´ë„ˆë›°ê¸°")
                        delta = ""
                except Exception as e:
                    logger.error(f"ë¸íƒ€ ê³„ì‚° ì˜¤ë¥˜: {e}")
                    delta = ""
                
                # 3. ë¸íƒ€ í’ˆì§ˆ ê²€ì¦ ë° ì „ì†¡
                if delta and not stop_found:
                    try:
                        cleaned_delta = self._validate_and_clean_delta(delta)
                        if cleaned_delta:
                            # JSON ì§ë ¬í™” ì•ˆì „ì„± ê°•í™”
                            json_data = json.dumps({'text': cleaned_delta}, ensure_ascii=False)
                            yield f"data: {json_data}\n\n"
                            last_text = current_text
                            logger.debug(f"ë¸íƒ€ ì „ì†¡ ì„±ê³µ: {len(cleaned_delta)}ì")
                        else:
                            logger.debug(f"ë¸íƒ€ í•„í„°ë§ë¨: '{delta[:20]}...'")
                    except Exception as e:
                        logger.error(f"ë¸íƒ€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
                
                # 4. ì¢…ë£Œ ì¡°ê±´ ì²´í¬
                if stop_found:
                    logger.info("ìŠ¤í†± í† í°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                    break
                    
                # 5-1. ìµœëŒ€ í† í° ìˆ˜ ì²´í¬
                if len(current_text.split()) >= safe_max_tokens:
                    logger.info(f"ìµœëŒ€ í† í° ìˆ˜({safe_max_tokens}) ë„ë‹¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")
                    break
                    
                # 5-2. ë¬¸ì ìˆ˜ ê¸°ë°˜ ì¢…ë£Œ (ëª¨ë¸ íƒ€ì…ë³„)
                max_chars = {
                    "autocomplete": 1500,   # ì•ˆì „ì„± ìš°ì„ 
                    "prompt": 3000,        # ì•ˆì „ì„± ìš°ì„ 
                    "comment": 1000,       # ì•ˆì „ì„± ìš°ì„ 
                    "error_fix": 2000      # ì•ˆì „ì„± ìš°ì„ 
                }.get(request.model_type, 2000)
                
                if len(current_text) >= max_chars:
                    logger.info(f"ë¬¸ì ìˆ˜ ì œí•œ({max_chars}) ë„ë‹¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ (current={len(current_text)})")
                    break
                
                # ì£¼ê¸°ì ì¸ ì§„í–‰ ìƒí™© ë¡œê·¸
                if token_count % 10 == 0:
                    logger.debug(f"ì§„í–‰ ìƒí™©: tokens={token_count}, chars={len(current_text)}, max_chars={max_chars}")
        
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì˜¤ë¥˜: {e}")
            try:
                error_data = json.dumps({"error": "ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ë°œìƒ"}, ensure_ascii=False)
                yield f"data: {error_data}\n\n"
            except Exception:
                yield "data: {\"error\": \"JSON ì§ë ¬í™” ì˜¤ë¥˜\"}\n\n"
        
        finally:
            # í•­ìƒ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ (ì •ìƒ/ë¹„ì •ìƒ ì¢…ë£Œ ë¬´ê´€)
            try:
                logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ - ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ (token_count={token_count})")
                completion_data = json.dumps({'type': 'done', 'text': ''}, ensure_ascii=False)
                yield f"data: {completion_data}\n\n"
            except Exception as e:
                logger.error(f"ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ ì˜¤ë¥˜: {e}")
                yield "data: {\"type\": \"done\", \"text\": \"\"}\n\n"

    async def generate_single(self, request: GenerateRequest) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        config = self.model_configs[request.model_type]
        stream_request = StreamGenerateRequest(
            user_id=0,
            prompt=request.prompt,
            model_type=request.model_type,
            user_select_options=request.user_select_options,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens if request.max_tokens is not None else config.max_tokens,
        )

        full_text = ""
        async for chunk in self.generate_stream(stream_request):
            # ìƒˆë¡œìš´ JSON í˜•ì‹ ì™„ë£Œ ì‹ í˜¸ ì²˜ë¦¬
            if chunk.strip().startswith("data: "):
                try:
                    data_str = chunk.strip()[5:].strip()
                    if data_str:
                        data = json.loads(data_str)
                        
                        # ì™„ë£Œ ì‹ í˜¸ ì²´í¬
                        if data.get("type") == "done":
                            logger.info("ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                            break
                            
                        # í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
                        if "text" in data and data["text"]:
                            full_text += data["text"]
                        elif "error" in data:
                            logger.error(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ ì˜¤ë¥˜ ìˆ˜ì‹ : {data['error']}")
                            return {'error': data['error']}
                except json.JSONDecodeError:
                    logger.warning(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ JSON ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ: {chunk}")
                    continue
            # êµ¬ì‹ [DONE] ì‹ í˜¸ í˜¸í™˜ì„± ìœ ì§€
            elif chunk.strip() == "data: [DONE]":
                logger.info("êµ¬ì‹ [DONE] ì‹ í˜¸ ìˆ˜ì‹ ")
                break
        return {'generated_text': full_text}

    def _validate_and_clean_delta(self, delta: str) -> str:
        """ì•ˆì „í•˜ê³  ì‹¤ìš©ì ì¸ ë¸íƒ€ ê²€ì¦ (ê³¼ë„í•œ í•„í„°ë§ ë°©ì§€)"""
        if not delta or len(delta.strip()) == 0:
            return ""
        
        import re
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        cleaned = delta.strip()
        
        # 2. ì‹¬ê°í•œ ì†ìƒë§Œ ì°¨ë‹¨ (ì˜ì–´ í…ìŠ¤íŠ¸ í—ˆìš©)
        severe_corruption_patterns = [
            r'([a-zA-Z])\1{5,}',  # ê°™ì€ ë¬¸ì 6ê°œ ì´ìƒ ë°˜ë³µ
            r'([a-zA-Z]{3,})\1{3,}',  # íŒ¨í„´ 4ë²ˆ ì´ìƒ ë°˜ë³µ
            r'[a-zA-Z]{3,}[0-9]{3,}[a-zA-Z]{3,}[0-9]{3,}',  # ì‹¬ê°í•œ ë¬¸ì-ìˆ«ì í˜¼ì¬
        ]
        
        for pattern in severe_corruption_patterns:
            if re.search(pattern, cleaned):
                logger.warning(f"ì‹¬ê°í•œ ì†ìƒ íŒ¨í„´ ê°ì§€: '{pattern}' in '{cleaned[:30]}...'")
                return ""  # ì‹¬ê°í•œ ì†ìƒë§Œ ì°¨ë‹¨
        
        # 3. ë¹„ì¸ì‡„ ê°€ëŠ¥ ë¬¸ì ì œê±° (ì œì–´ ë¬¸ìë§Œ)
        cleaned = ''.join(c for c in cleaned if c.isprintable() or c.isspace())
        
        # 4. ê²½ë¯¸í•œ ì¤‘ë³µ íŒ¨í„´ ì œê±° (ì•ˆì „í•œ ì •ê·œì‹)
        try:
            # ê°™ì€ ë¬¸ì 3ê°œ ì´ìƒ ë°˜ë³µë§Œ ì œê±°
            cleaned = re.sub(r'([a-zA-Z_])\1{2,}', r'\1\1', cleaned)
        except re.error as e:
            logger.warning(f"ì •ê·œì‹ ì˜¤ë¥˜: {e}, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©")
            pass
        
        # 5. ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(cleaned.strip()) < 1:
            return ""
        
        # 6. ë§¤ìš° ê´€ëŒ€í•œ ê²€ì¦ (ê±°ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í—ˆìš©)
        # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆê³  ìµœì†Œí•œì˜ ë‚´ìš©ì´ ìˆìœ¼ë©´ í†µê³¼
        if len(cleaned.strip()) > 0:
            logger.debug(f"ë¸íƒ€ í†µê³¼: '{cleaned[:30]}...' (ê¸¸ì´: {len(cleaned)})")
            return cleaned
        
        return ""