# app/engine.py
"""
vLLM ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  í…ìŠ¤íŠ¸ ìƒì„± ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì½”ì–´ ëª¨ë“ˆ.
VRAM ìµœì í™”, ë²ˆì—­ ì„œë¹„ìŠ¤ ì—°ë™, ìƒì„± ì¤‘ë‹¨ ë¡œì§ì´ ëª¨ë‘ í¬í•¨ëœ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.
"""
import logging
import time
import json
from typing import Dict, List, Any, AsyncGenerator
import httpx  # API í˜¸ì¶œì„ ìœ„í•œ httpx

from fastapi import HTTPException
from transformers import AutoTokenizer
from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.lora.request import LoRARequest

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from .config import ModelConfig
from .schemas import GenerateRequest, StreamGenerateRequest
# âœ¨ prompt_builder.pyë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
# from .prompt_builder import generate_enhanced_prompt

logger = logging.getLogger(__name__)

# ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ ì£¼ì†Œ (docker-compose ì„œë¹„ìŠ¤ ì´ë¦„ ê¸°ì¤€)
TRANSLATOR_API_URL = "http://translator:8003/translate"

async def call_translation_service(korean_prompt: str) -> str:
    """ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    logger.info(f"ë²ˆì—­ ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œì‘: '{korean_prompt}'")
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(TRANSLATOR_API_URL, json={"text": korean_prompt})
            response.raise_for_status()
            data = response.json()
            translated_text = data.get('translated_text', korean_prompt)
            logger.info(f"ë²ˆì—­ ì™„ë£Œ: -> '{translated_text}'")
            return translated_text
    except httpx.RequestError as e:
        logger.error(f"ë²ˆì—­ ì„œë¹„ìŠ¤({e.request.url}) í˜¸ì¶œ ì‹¤íŒ¨: {e}. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return korean_prompt

class VLLMMultiLoRAEngine:
    """vLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—”ì§„ í´ë˜ìŠ¤"""

    def __init__(self, base_model_path: str, model_configs: Dict[str, ModelConfig]):
        self.base_model_path = base_model_path
        self.model_configs = model_configs
        self.engine = None
        self.tokenizer = None
        self.is_initialized = False

    def initialize_engine(self):
        """vLLM ë¹„ë™ê¸° ì—”ì§„ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ vLLM ë¹„ë™ê¸° ì—”ì§„ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘ (VRAM ìµœì í™” ëª¨ë“œ)...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            
            # âœ¨ VRAM ìµœì í™” ë° ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ìµœì¢… EngineArgs ì„¤ì •
            engine_args = AsyncEngineArgs(
                model=self.base_model_path,
                quantization="marlin",
                enable_lora=True,
                trust_remote_code=True,
                gpu_memory_utilization=0.4,
                max_model_len=2048,
                max_num_seqs=4,
                tokenizer_mode="slow",
                max_loras=5,
                max_lora_rank=32,
                dtype="half",
                tensor_parallel_size=1,
            )
            
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            self.is_initialized = True
            logger.info("âœ… ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (VRAM ìµœì í™” ëª¨ë“œ)")
        except Exception as e:
            logger.error(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self.is_initialized = False
            raise

    async def generate_stream(self, request: StreamGenerateRequest) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_initialized or self.engine is None or self.tokenizer is None:
            raise HTTPException(status_code=503, detail="Model engine not initialized")

        if request.model_type not in self.model_configs:
            error_msg = f"Model type '{request.model_type}' not configured."
            logger.error(error_msg)
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
            return
            
        english_prompt = await call_translation_service(request.prompt)

        config = self.model_configs[request.model_type]
        
        # âœ¨ ìƒì„¸ í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ë‹¨ìˆœ í…œí”Œë¦¿ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        messages = [{"role": "system", "content": config.system_prompt}, {"role": "user", "content": english_prompt}]
        final_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        sampling_params = SamplingParams(
            temperature=request.temperature if request.temperature is not None else config.temperature,
            top_p=request.top_p if request.top_p is not None else config.top_p,
            max_tokens=request.max_tokens,
            stop=config.stop or [],
            repetition_penalty=1.15,
            include_stop_str_in_output=False  # stop í† í°ì„ ì¶œë ¥ì—ì„œ ì œì™¸í•˜ì—¬ FIM íƒœê·¸ ì¸ì‹ ê°œì„ 
        )
        
        # FIM íƒœê·¸ ì¸ì‹ì„ ìœ„í•œ ë¡œê¹…
        logger.info(f"Stop tokens for {request.model_type}: {config.stop}")

        lora_request = LoRARequest(lora_name=config.name, lora_int_id=config.lora_id, lora_path=config.adapter_path)
        request_id = f"stream-{int(time.time() * 1000)}"
        results_generator = self.engine.generate(final_prompt, sampling_params, request_id, lora_request)

        # âœ¨ ìƒì„± ì¤‘ë‹¨(Stop Token) ì˜¤ë¥˜ë¥¼ í•´ê²°í•œ ìµœì¢… ìŠ¤íŠ¸ë¦¬ë° ë¡œì§
        last_text = ""
        async for request_output in results_generator:
            text_so_far = request_output.outputs[0].text
            
            stop_found = False
            stop_str_found = ""
            for stop_str in (config.stop or []):
                if stop_str in text_so_far:
                    # Stop Token ì´ì „ê¹Œì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ìë¦„
                    text_so_far = text_so_far.split(stop_str)[0]
                    stop_found = True
                    stop_str_found = stop_str
                    break
            
            delta = text_so_far[len(last_text):]
            if delta:
                yield f"data: {json.dumps({'text': delta})}\n\n"
            
            last_text = text_so_far

            if stop_found:
                logger.info(f"Stop token '{stop_str_found}' ê°ì§€ë¨. ìŠ¤íŠ¸ë¦¼ì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

        yield "data: [DONE]\n\n"

    async def generate_single(self, request: GenerateRequest) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        config = self.model_configs[request.model_type]
        stream_request = StreamGenerateRequest(
            user_id=0,
            prompt=request.prompt,
            model_type=request.model_type,
            user_select_options=request.user_select_options,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens if request.max_tokens is not None else config.max_tokens,
        )

        full_text = ""
        async for chunk in self.generate_stream(stream_request):
            if chunk.strip() == "data: [DONE]":
                break
            try:
                data_str = chunk.strip()[5:].strip()
                if data_str:
                    data = json.loads(data_str)
                    if "text" in data:
                        full_text += data["text"]
                    elif "error" in data:
                        logger.error(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ ì˜¤ë¥˜ ìˆ˜ì‹ : {data['error']}")
                        return {'error': data['error']}
            except json.JSONDecodeError:
                logger.warning(f"ìŠ¤íŠ¸ë¦¼ì—ì„œ JSON ë””ì½”ë”© ì˜¤ë¥˜ ë°œìƒ: {chunk}")
                continue
        return {'generated_text': full_text}