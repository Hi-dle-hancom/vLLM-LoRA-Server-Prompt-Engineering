# app/main.py
"""
FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ íŒŒì¼.
ì•± ìƒì„±, ë¯¸ë“¤ì›¨ì–´ ì„¤ì •, ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬, API ë¼ìš°íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""
import logging
import time

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from prometheus_fastapi_instrumentator import Instrumentator

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from .config import BASE_MODEL_PATH, MODEL_CONFIGS
from .engine import VLLMMultiLoRAEngine
from .schemas import GenerateRequest, StreamGenerateRequest

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- FastAPI ì•± ë° vLLM ì—”ì§„ ì´ˆê¸°í™” ---
app = FastAPI(title="vLLM Multi-LoRA Server", version="1.0.0")
engine = VLLMMultiLoRAEngine(BASE_MODEL_PATH, MODEL_CONFIGS)

# --- ë¯¸ë“¤ì›¨ì–´ ì„¤ì • ---
# ğŸ” ë³´ì•ˆ ê°•í™”: ëª…ì‹œì  ë„ë©”ì¸ ëª©ë¡ (ë‹¤ë¥¸ ì„œë²„ì™€ ì¼ì¹˜)
origins = [
    # EC2 ì„œë²„ (ê³µì¸ IP)
    "http://3.13.240.111:3000",  # React Landing Page
    "http://3.13.240.111:8000",  # Backend API
    "http://3.13.240.111:8001",  # DB Module
    "http://3.13.240.111:8002",  # vLLM Server (ìê¸° ìì‹ )
    "http://3.13.240.111:8003",  # Translator Service
    
    # ë¡œì»¬ ê°œë°œ í™˜ê²½
    "http://localhost:3000",     # React ê°œë°œìš©
    "http://localhost:8000",     # Backend API
    "http://localhost:8001",     # DB Module
    "http://localhost:8002",     # vLLM Server
    "http://localhost:8003",     # Translator Service
    
    # 127.0.0.1 ë¡œì»¬í˜¸ìŠ¤íŠ¸
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8000",
    "http://127.0.0.1:8001",
    "http://127.0.0.1:8002",
    "http://127.0.0.1:8003",
    
    # VSCode Extension ì§€ì›
    "vscode://",                 # VSCode Extension
    "vscode-webview://*"         # VSCode WebView
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Prometheus ë©”íŠ¸ë¦­ ì„¤ì •
instrumentator = Instrumentator().instrument(app)

# --- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ vLLM ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  Prometheus ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤."""
    instrumentator.expose(app)
    try:
        engine.initialize_engine()
        logger.info("âœ… FastAPI ì„œë²„ ì‹œì‘, /metrics ì—”ë“œí¬ì¸íŠ¸ í™œì„±í™”")
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}", exc_info=True)
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì—¬ê¸°ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•˜ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        raise RuntimeError(f"Engine initialization failed: {e}") from e

# --- API ì—”ë“œí¬ì¸íŠ¸ (ë¼ìš°íŠ¸) ---
@app.get("/health", tags=["Health"])
async def health_check():
    """ì„œë²„ì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    if engine.is_initialized:
        return {"status": "healthy", "timestamp": time.time()}
    else:
        raise HTTPException(status_code=503, detail="Service unavailable - model not initialized")

@app.get("/", tags=["Info"])
async def root():
    """ì„œë²„ ì •ë³´ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        "service": "vLLM Multi-LoRA Server",
        "version": app.version,
        "status": "running" if engine.is_initialized else "initializing",
        "available_models": list(MODEL_CONFIGS.keys()) if engine.is_initialized else []
    }

@app.post("/generate", tags=["Generation"])
async def generate(request_data: GenerateRequest):
    """ë‹¨ì¼ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        if request_data.model_type not in MODEL_CONFIGS:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid model_type. Available types: {list(MODEL_CONFIGS.keys())}"
            )
        return await engine.generate_single(request_data)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/generate/stream", tags=["Generation"])
async def generate_stream(request_data: StreamGenerateRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        if request_data.model_type not in MODEL_CONFIGS:
             # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œë„ ì˜¤ë¥˜ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬
            async def error_stream():
                error_msg = f"Invalid model_type. Available types: {list(MODEL_CONFIGS.keys())}"
                yield f"data: {json.dumps({'error': error_msg})}\n\n"
            return StreamingResponse(error_stream(), media_type="text/event-stream", status_code=400)

        return StreamingResponse(
            engine.generate_stream(request_data),
            media_type="text/event-stream"
        )
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œë„ ì˜¤ë¥˜ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬
        async def exception_stream():
            yield f"data: {json.dumps({'error': f'Internal server error: {str(e)}'})}\n\n"
        return StreamingResponse(exception_stream(), media_type="text/event-stream", status_code=500)